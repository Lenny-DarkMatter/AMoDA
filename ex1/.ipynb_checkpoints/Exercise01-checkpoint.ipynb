{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Maximum-likelihood fitting\n",
    "\n",
    "In this exercise we will implement a maximum-likelihood fit using the common example of fitting a Gaussian curve to some normally-distributed data.\n",
    "\n",
    "The main task will be to implement a gradient-descent algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "The objective of fitting a PDF to data is to find the values of the PDF's parameters that maximise the likelihood, thus giving the best description of a dataset (i.e. the best fit).\n",
    "\n",
    "While it's possible to store parameters as simple `float`s, we ask that you implement a class that stores the parameter value and a range that the value cannot stray outside during the fit. This will be especially helpful while testing and debugging your minimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDFs and likelihoods\n",
    "\n",
    "A Gaussian PDF is defined as:\n",
    "$$\n",
    "f(x|\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "The likelihood is the product of the PDF evaluated on all points $\\{x_i\\}$ in a dataset (assuming all points are independent):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mu,\\sigma) = \\prod_i f(x_i|\\mu,\\sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than maximise the likelihood, it is easier computationally to minimise the negative log-likelihood:\n",
    "$$\n",
    "-\\log(\\mathcal{L}(\\mu,\\sigma)) = -\\sum_i\\log(f(x_i|\\mu,\\sigma)) \n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\log(\\mathcal{L}(\\mu,\\sigma)) = \\log\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right) + \\frac{1}{2\\sigma^2} \\sum_i \\left(-(x_i-\\mu)^2\\right) =: -L(\\mu, \\sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(x, mu, sig):\n",
    "    fac = np.log(1 / (np.sqrt(2*np.pi)*sig) )\n",
    "    exp = np.sum((x-mu)**2) / (2*sig**2)\n",
    "    return fac+exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimisation via gradient descent\n",
    "\n",
    "Gradient descent is a method of finding the local minimum of a differentiable function by following the local gradient.\n",
    "\n",
    "Note that the local minimum may not be the same as the global minimum. There are strategies to avoid a local-but-not-global minimum, such as performing a fit multiple times with random starting values.\n",
    "\n",
    "Say we have a multivariate function $L(\\vec{\\theta})$, where $\\vec{\\theta} = [\\theta_1, \\theta_2, ..., \\theta_n].$ _**NB**: in this exercise, $L(\\vec{\\theta})=-\\log(\\mathcal{L}(\\mu,\\sigma))$, the negative log-likelihood._\n",
    "\n",
    "The gradient of the function is $$\\nabla_\\theta L(\\vec{\\theta}) = \\left[\\frac{\\partial L}{\\partial \\theta_1}, \\frac{\\partial L}{\\partial \\theta_2}, ..., \\frac{\\partial L}{\\partial \\theta_n}\\right].$$\n",
    "\n",
    "Starting from some initial position $\\vec{\\theta}_i$, one can descend the gradient towards the minimum by subtracting an amount proportional to the local gradient: $$\\vec{\\theta}_{t} = \\vec{\\theta}_{t-1} - \\eta_t \\nabla_\\theta L(\\vec{\\theta}_{t-1}),$$ where $\\eta_t$ is the \"step size\". This can be repeated until $\\vec{\\theta}$ converges on the values that minimise $L(\\vec{\\theta})$ (i.e. the best-fit values $\\vec{\\hat{\\theta}}$).\n",
    "\n",
    "The exact criterion for achieving convergence is up to you to decide and implement.\n",
    "A good starting point is to use the relative change $$\\left|\\frac{L(\\vec{\\theta}_{i})-L(\\vec{\\theta}_{i-1})}{L(\\vec{\\theta}_{i})+L(\\vec{\\theta}_{i-1})}\\right|$$ and stop when it goes below some threshold.\n",
    "\n",
    "Choosing an appropriate step size $\\eta_t$ is crucial for an optimal balance between speed and precision, and many methods are available for doing this. Note that it does not need to be a fixed size and can be adjusted at each iteration.\n",
    "\n",
    "### Batch vs stochastic\n",
    "\n",
    "In batch gradient descent, the parameters are updated using the likelihood calculated over the full dataset.\n",
    "\n",
    "In stochastic gradient descent, the parameters are updated for each datapoint (using $\\nabla_\\theta L(\\vec{\\theta}, x_i)$), or a sub-sample of the full dataset (mini-batch).\n",
    "\n",
    "*See the lecture notes for more.*\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Useful particularly in stochastic/mini-batch gradient descent is the idea of 'momentum'. Where the parameters are updated using: $$\\vec{\\theta}_t = \\vec{\\theta}_{t-1} - \\vec{v}_t,$$ where $$\\vec{v}_t = \\gamma \\vec{v}_{t-1} + \\eta_t \\nabla_\\theta L(\\vec{\\theta}_{t-1}, x),$$ where $\\gamma$ is the momentum parameter $0 < \\gamma < 1$ (typically around 0.9).\n",
    "\n",
    "### Nesterov's accelerated gradient descent\n",
    "\n",
    "Using momentum we can 'look ahead' to where the next update will be approximately, without calculating a new gradient: $$\\vec{\\theta}_t \\approx \\vec{\\theta}_{t-1} - \\gamma \\vec{v}_{t-1}$$\n",
    "\n",
    "We can instead use that position when calculating the new gradient, so $\\vec{v}_t$ becomes: $$\\vec{v}_t = \\gamma \\vec{v}_{t-1} + \\eta_t \\nabla_\\theta L(\\vec{\\theta}_{t-1} - \\gamma \\vec{v}_{t-1}, x).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing your own gradient-descent minimiser\n",
    "\n",
    "Here you should implement:\n",
    "1. A parameter class that holds the value and allowed range of a parameter\n",
    "  - When setting a value outside the allowed range, force the value to equal the nearest boundary\n",
    "2. A Gaussian PDF class or function using parameters that control its mean and standard deviation\n",
    "3. A gradient-descent minimiser which iteratively:\n",
    "  - Calcualtes the likelihood gradient at the current values of the fit parameters\n",
    "  - Updates the parameters following the gradient\n",
    "  - Saves the likelihood in a list (for plotting later)\n",
    "  - Stops the fit if it has converged (or if a maximum number of iterations have been reached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(func, derive_dim, x, *args):\n",
    "    import numpy as np\n",
    "    eps = np.finfo(float).eps\n",
    "    hi = np.array(args, dtype=float)\n",
    "    lo = np.array(args, dtype=float)\n",
    "    hi[derive_dim] *= (1+eps)\n",
    "    lo[derive_dim] *= (1-eps)\n",
    "    \n",
    "    dv = ( func(x, *hi) - func(x, *lo) ) / (2*args[derive_dim]*eps)\n",
    "    \n",
    "    return dv\n",
    "    \n",
    "\n",
    "def gradient(func, x, *args):\n",
    "    out = []\n",
    "    for i in range(len(args)):\n",
    "        out.append(derivative(func, i, x, args))\n",
    "    \n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the parameter class, the Guassian PDF and gradient-descent minimisation here\n",
    "# You can create extra cells if you wish\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# def grad(val_arr, x_arr=None):\n",
    "#     \"\"\"\n",
    "#     calculates gradient 'dy/dx' with \n",
    "    \n",
    "#     params:\n",
    "#     x_arr : dx, numpy-array\n",
    "#     val_arr : dy, numpy-array\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # if no dx is given, assume equal distances\n",
    "#     if type(x_arr) != np.ndarray:\n",
    "#         x_arr = val_arr * 0 + 1\n",
    "    \n",
    "#     # linear distance on x and y axis\n",
    "#     diffx = x_arr[1:] - x_arr[:-1]\n",
    "#     diffx = np.append(diffx[0], diffx)\n",
    "    \n",
    "#     diffy = val_arr[1:] - val_arr[:-1]\n",
    "#     diffy = np.append(diffy[0], diffy)\n",
    "    \n",
    "#     # gradient at dx/2 (or half a step on x-axis) before the array values\n",
    "#     # \"pregradient\"\n",
    "#     pregrad = diffy / diffx\n",
    "    \n",
    "#     # linearly interpolate gradient between dy/dx before and after array values\n",
    "#     gradient = (pregrad[1:] + pregrad[:-1]) / 2\n",
    "#     gradient = np.append(pregrad[0], gradient)\n",
    "    \n",
    "#     return gradient\n",
    "\n",
    "class param:\n",
    "    def __init__(self, value, lower_limit, upper_limit):\n",
    "        self.low = lower_limit\n",
    "        self.up = upper_limit\n",
    "        self.value = value\n",
    "        return None\n",
    "        \n",
    "    def new_value(self, new_val):\n",
    "        # including a range check\n",
    "        if new_val < self.low:\n",
    "            self.value = self.low\n",
    "        elif new_val > self.up:\n",
    "            self.value = self.up\n",
    "        else:\n",
    "            self.value = new_val\n",
    "        return None\n",
    "            \n",
    "        \n",
    "def gauss(x, mu, sig):\n",
    "    return 1 / np.sqrt(2*np.pi*sig**2) * np.exp( -(x-mu)**2 / (2*sig**2) )\n",
    "\n",
    "\n",
    "\n",
    "# def chisq(x, mu, sig):\n",
    "#     return npd(self):\n",
    "#         return 1 / np.sqrt(2*np.pi*self.sigma**2) * np.exp( -(self.x-self.mu)**2 / (2*self.sigma**2) )\n",
    "#       # include gradient here!\n",
    "\n",
    "# 3. A gradient-descent minimiser which iteratively:\n",
    "  # - Calcualtes the likelihood gradient at the current values of the fit parameters\n",
    "  # - Updates the parameters following the gradient\n",
    "  # - Saves the likelihood in a list (for plotting later)\n",
    "  # - Stops the fit if it has converged (or if a maximum number of iterations have been reached)\n",
    "\n",
    "\n",
    "\n",
    "def gdMinimiser(data, eta=0.9):\n",
    "    \"\"\"\n",
    "    maximise a function's log-likelihood (minimise chisq)\n",
    "    using stochastic gradient descent (assumes normal distributed values)\n",
    "    \n",
    "    params:\n",
    "    data : array of data points\n",
    "    eta : learning rate\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # mu0, sigma0 = data.mean(), data.std()\n",
    "    # self.mu0 = mu0\n",
    "    # self.sigma0 = sigma0\n",
    "\n",
    "    # if mu would not lie in data, maybe to little values\n",
    "    mu_min, mu_max = data.min(), data.max()\n",
    "\n",
    "    # not so sure yet, maybe mu_min, mu_max\n",
    "    sigma_min, sigma_max = mu_min, mu_max\n",
    "    \n",
    "    # define 't-1' parameters with small offset to 't'\n",
    "    mu0  = param(data.mean() *1.2,  mu_min,  mu_max)\n",
    "    sigma0 = param(data.std() *1.2, sigma_min, sigma_max)\n",
    "    \n",
    "    # define 't' parameters\n",
    "    mu1  = param(data.mean(), mu_min, mu_max)\n",
    "    sigma1 = param(data.std(), sigma_min, sigma_max)\n",
    "    \n",
    "    # Saves the likelihood in a list (for plotting later)\n",
    "    lhood_list = []\n",
    "\n",
    "    # start and handling params for while loop\n",
    "    # Stops the fit if it has converged (or if a maximum number of iterations have been reached)\n",
    "    lhood = 1e10\n",
    "    n_iter = 0\n",
    "    max_iter = 5e4\n",
    "    \n",
    "    ###########\n",
    "    mu_list, sigma_list = [], []\n",
    "    \n",
    "    while lhood > 2 and n_iter < max_iter:\n",
    "        # find gradient of lhood-function, use difference between lhood of new and old parameter value,\n",
    "        # while all other params stay fixed as the new one\n",
    "        lhood = chisq(data, mu1.value, sigma1.value)\n",
    "        gradient_mu  = lhood - chisq(data, mu0.value, sigma1.value)\n",
    "        gradient_sigma = lhood - chisq(data, mu1.value, sigma0.value)\n",
    "        \n",
    "        # save for plotting\n",
    "        lhood_list.append(lhood)\n",
    "\n",
    "        mu0.new_value(mu1.value)\n",
    "        sigma0.new_value(sigma1.value)\n",
    "        \n",
    "        mu1.new_value(mu1.value - eta * gradient_mu)\n",
    "        sigma1.new_value(sigma1.value - eta * gradient_sigma)\n",
    "        \n",
    "        mu_list.append(mu1.value)\n",
    "        sigma_list.append(np.sqrt(sigma1.value))\n",
    "        \n",
    "        n_iter += 1\n",
    "        \n",
    "    if n_iter < max_iter:\n",
    "        print(f\"Converged after {n_iter} iterations!\")\n",
    "    # else:\n",
    "    #     raise OptimizeWarning(f\"Did not converge, maximum number of iterations ({max_iter}) reached.\\nmu = {mu1.value}, sigma = {var1.value}\")\n",
    "   \n",
    "    return [mu1.value, sigma1.value], lhood_list, mu_list, sigma_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5593.625398457925, 4994.609457459292]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p1 = param(10, 1.5, 4.5)\n",
    "# p1.value\n",
    "\n",
    "# percent = 0\n",
    "# for i in range(100000):\n",
    "#     percent += gauss(130+i/100, 100, 15).value() * 0.01\n",
    "\n",
    "# percent*100\n",
    "\n",
    "# test_arr =   np.array([0,  2, 4, 6, 8, 12])\n",
    "# test_arr_x = np.array([-2, 0, 1, 2, 3, 5])\n",
    "# print(test_arr, test_arr_x)\n",
    "# print(np.gradient(test_arr, test_arr_x), grad(test_arr, test_arr_x))\n",
    "\n",
    "\n",
    "popt, lhoods, mus, sigs = gdMinimiser(data, eta=0.1)\n",
    "# plt.plot(lhoods)\n",
    "# plt.show()\n",
    "# plt.plot(mus)\n",
    "popt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.31295031e+03, -1.03045330e-02])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.optimize as so\n",
    "\n",
    "num_bins = 50\n",
    "vals, bins, _ = plt.hist(df[\"mass\"], num_bins)\n",
    "bin_width = (bins[-1] - bins[0])/num_bins\n",
    "\n",
    "x = (bins[1:] + bins[:-1]) / 2\n",
    "y = vals\n",
    "y_err = vals\n",
    "y_err[y_err == 0] = 1\n",
    "y_err = np.sqrt( y_err )\n",
    "\n",
    "so.curve_fit(gauss, x, y, sigma=y_err, p0=[data.mean(), data.std()**2])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14e414520>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting data\n",
    "\n",
    "The data sample provided with this sheet consists of Gaussian-distributed measurements of the $B^0$ meson mass, in units of MeV. The standard deviation of the distribution is dominated by the resolution of the detector used to make the measurement.\n",
    "\n",
    "Your task is to obtain best-fit values for the mass of the meson (i.e. the mean, $\\mu$) and the detector resolution (i.e. the standard deviation $\\sigma$) using your minimiser and Gaussian PDFs implemented above.\n",
    "\n",
    "In the cell below, the data is loaded into a [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) and plotted as a [Matplotlib histogram](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html). This should give you an idea of which starting values and ranges to set for the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWp0lEQVR4nO3de5gldX3n8ffHQQWvYcKA42UcNXjBCxAnasK6i6AGgQDeAFd92MR1Nrsx6yYxZlzcyK7ZLBJNYjZGHSM68YKiiBCiKJkVjFcYFBBFheCIyCwD3jC7BAS++0fV7Bya7j5nerpOn9P1fj1PP6fqd6pOfb/T8D3Vv/rVr1JVSJL6415LHYAkabws/JLUMxZ+SeoZC78k9YyFX5J6Zo+lDmAU++yzT61du3apw5CkqXLppZfeXFWrZrZPReFfu3YtW7ZsWeowJGmqJPnubO129UhSz1j4JalnLPyS1DMWfknqGQu/JPWMhV+SesbCL0k9Y+GXpJ6x8EtSz0zFnbvSuKzd8Heztm899agxRyJ1xzN+SeoZC78k9YyFX5J6xsIvST1j4ZeknrHwS1LPOJxT2g0O/9Q08oxfknrGwi9JPWPhl6SesfBLUs9Y+CWpZyz8ktQzFn5J6hkLvyT1jIVfknrGwi9JPWPhl6SesfBLUs9Y+CWpZyz8ktQznU7LnGQr8FPgTuCOqlqXZCXwYWAtsBU4vqp+1GUckqSdxnHG/6yqOqiq1rXrG4DNVbU/sLldlySNyVJ09RwLbGqXNwHHLUEMktRbXT+Bq4BPJyngnVW1EdivqrYBVNW2JPvOtmOS9cB6gDVr1nQcpqadT8KSRtd14T+kqm5oi/sFSb456o7tl8RGgHXr1lVXAUpS33Ta1VNVN7Sv24GzgacBNyZZDdC+bu8yBknS3XVW+JPcP8kDdywDzwWuBM4FTmo3Owk4p6sYJEn31GVXz37A2Ul2HOeDVXV+kkuAM5O8ArgOeHGHMUiSZuis8FfVtcCBs7T/ADi8q+NKkubnnbuS1DMWfknqGQu/JPWMhV+SesbCL0k90/Wdu5JmWKzpJZymQgvlGb8k9YyFX5J6xsIvST1j4ZeknrHwS1LPWPglqWcczqllzSGP0j15xi9JPWPhl6SesfBLUs9Y+CWpZyz8ktQzFn5J6hkLvyT1zEjj+JPsCxwCPBS4FbgS2FJVd3UYmySpA/MW/iTPAjYAK4GvAtuBPYHjgMck+Sjwlqq6peM4JUmLZNgZ/5HAK6vquplvJNkDOBp4DnBWB7FJkjowrPC/uapunO2NqroD+PiiRyT1lNNLaFyGXdy9PMkFSX4jyYPHEpEkqVPDCv/DgDcDzwS+neTjSU5Islf3oUmSujBv4a+qO6vqU1X168AjgPfQXNj9TpIPjCE+SdIiG3kcf1XdDnwDuAq4BThglP2SrEjy1STntesr2+6jq9vXvRcSuCRpYYYW/iRrkvx+kq8A5wErgGOr6uARj/Fqmi+LHTYAm6tqf2Bzuy5JGpN5C3+SLwD/AOwHrK+qx1XVG6rqqvn2G9j/4cBRwF8PNB8LbGqXN9F0HUmSxmTYcM7XAZ+tqlrg5/858FrggQNt+1XVNoCq2tbeFXwPSdYD6wHWrFmzwMNruZlryONSfc44TFOsmg7DLu5eVFWV5LFJNie5EiDJU5K8fr59kxwNbK+qSxcSWFVtrKp1VbVu1apVC/kISdIsRr24+y6as/+fAVTVFcCJQ/Y5BDgmyVbgQ8BhSd4P3JhkNUD7un0BcUuSFmjUwn+/qrp4Rtsd8+1QVa+rqodX1VqaL4n/VVUvA84FTmo3Owk4ZxfilSTtppFm5wRuTvIYoACSvAjYtsBjngqcmeQVwHXAixf4OdLY2M+u5WTUwv9bwEbg8Um+D3wHeNmoB6mqC4EL2+UfAIfvUpSSpEUzUuGvqmuBZye5P3Cvqvppt2FJkroybD7+352jHYCq+tMOYpIkdWjotMzAZcAngduAdB2QJKlbwwr/L9KMyDkKuBQ4g2a6hYXe0CVJWmLDbuC6rKo2VNVBwLtpplv4RpJjxhGcJGnxjTSOP8kq4GDgycD1eNOVJE2tYRd3fx04geYB6x8Fjq8qi74kTbFhffzvBr5Gc6PVrwLP3TGiB6Cq7PKRpCkzrPA/ayxRSJLGZt7CX1UXjSsQ9dNcUyFsPfWoMUeyuJziQZNs5EcvSpKWBwu/JPWMhV+SembYcM49gFcAzwceSjMt8w00c+i/u6p+1nmEkqRFNWxUz/uAHwOn0Ny4BfBwmgeovJ9mjL8kaYoMnaunqh43o+164EtJvt1RTJKkDg0r/D9K8mLgrKq6CyDJvWiemvWjroOT1L1dHXo67UNtNfzi7onAi2gekP7t9iz/fwMvYPjD1iVJE2jYDVxbafvxk/w8kKq6eQxxSZI6MvJwzqr6wWDRT/KQbkKSJHVpd8bxv3vRopAkjc2CC39VeYVHkqaQd+5KUs/MW/iTPDnJl5J8L8nGJHsPvHdx9+FJkhbbsDP+t9Pctftk4NvA55I8pn3v3h3GJUnqyLAbuB5QVee3y29OcilwfpKX08zbI0maMsMKf5I8uKp+AlBVn0nyQuAsYGXn0UmSFt2wwv8m4AnAl3Y0VNUVSQ4H/kuXgWn6zHfrv7f5S5Nj2J27H5yj/TrglfPtm2RP4LPAfdvjfLSq3pBkJfBhYC2wFTi+qpz3R5LGZMHDOZNsHLLJbcBhVXUgcBBwRJJnABuAzVW1P7C5XZckjcmwB7HM1Y8f4Mj59q2qAv6pXb13+1PAscChbfsm4ELgD0aKVpK024b18d8EfJem0O9Q7fq+wz48yQrgUuAXgLdV1ZeT7FdV2wCqaluSWT8nyXpgPcCaNWuGHUrLzK5OFayd5vq38zqLdhhW+K8FDm/79O8myfeGfXhV3QkclOTngLOTPGnUwKpqI7ARYN26dQ4dlaRFMqyP/8+Bved477RRD1JVP6bp0jmCZm7/1QDt6/ZRP0eStPuGjep52zzv/c/59k2yCvhZVf04yV7As2mGh55L88zeU9vXc3Y1aEm7zu4z7TDs4u6/qKrPzfP+g4A1VXXlLG+vBja1/fz3As6sqvOSfBE4M8krgOtoHuMoSRqTYX38L0xyGnA+zUXam4A9aS7WPgt4JPB7s+1YVVcAB8/S/gPg8N2IWZK0G4Z19fxOOyPni2jOzFcDtwJXAe+c768BSdJkGnbGT3tX7bvaH0nSlPNBLJLUMxZ+SeqZYU/gWj2uQCRJ4zGsj//09uLuhTQjez5XVXd0HpUkqTPDRvU8r51e+VDg+TRP4bqO5kvg/NmmcpAkTbZRRvX8M22hB0jyKOB5wF8meUhVPa3bECVJi2lo4Z+pqr4D/BXwV0nus/ghSZK6tFujeqrq9sUKRJI0Hg7nlKSe2eXCn2TvJE/pIhhJUvdG6uNPciFwTLv9ZcBNSS6qqt/tLjRJk8gnfE2/Uc/4H1xVtwAvAN5TVU+lmV9fkjRlRi38e7R38R4PnNdhPJKkjo1a+P8r8Cngmqq6JMmjgau7C0uS1JVRx/Fvq6r/f0G3qq5N8qcdxaRlyMf+SZNj1DP+2Z6vO+8zdyVJk2nYM3d/GfgVYFWSwRE8DwJWdBmYJKkbw7p67gM8oN3ugQPtt9A8jlGSNGWGzc55EXBRkvdW1XfHFJMkqUOjXty9b5KNwNrBfarqsC6CkiR1Z9TC/xHgHcBfA3d2F44kqWujFv47qurtnUYiSRqLUYdz/m2S/5BkdZKVO346jUyS1IlRz/hPal9/f6CtgEcvbjiSpK6NVPir6lFdByJJGo+RunqS3C/J69uRPSTZP8nRQ/Z5RJLPJLkqydeTvLptX5nkgiRXt697734akqRRjdrH/x7gdpq7eAGuB/5oyD53AL9XVU8AngH8VpIDgA3A5qraH9jcrkuSxmTUwv+YqjoN+BlAVd0KZL4dqmpbVX2lXf4pcBXwMOBYYFO72SbguF0PW5K0UKMW/tuT7EVzQZckjwFuG/UgSdYCBwNfBvarqm3QfDkA+86xz/okW5Jsuemmm0Y9lCRpiFEL/ynA+cAjknyApovmtaPsmOQBwFnAf2qf4jWSqtpYVeuqat2qVatG3U2SNMSoo3o+neRSmr76AK+uqpuH7Zfk3jRF/wNV9bG2+cYkq6tqW/tUr+0LjF2StACjjuo5F3gucGFVnTdi0Q/wbuCqqhp8aMu57Lwv4CTgnF0LWZK0O0bt6nkL8EzgG0k+kuRFSfYcss8hwMuBw5Jc1v4cCZwKPCfJ1cBz2nVJ0piM2tWzY3rmFcBhwCuB02keyDLXPp9j7pE/h+9inJKkRTLqlA20o3p+DTgB+EV2DsmUJE2RkQp/kg8DT6cZ2fM2mr7+u7oMTJPLB6drNnP9d7H11KPGHImGGfWM/z3Av64q5+KXpCk378XdJK8FqKrzgRfMeO+PO4xLktSRYaN6ThxYft2M945Y5FgkSWMwrPBnjuXZ1iVJU2BY4a85lmdblyRNgWEXdw9McgvN2f1e7TLt+rAbuCRJE2jewl9VK8YViCRpPEa+gUv943h9aXkada4eSdIyYeGXpJ6x8EtSz1j4JalnLPyS1DMWfknqGYdzSloS8w0XdirnbnnGL0k9Y+GXpJ6x8EtSz9jHL6lTC5n6w8c4dsszfknqGQu/JPWMXT1yFk6pZzzjl6SesfBLUs9Y+CWpZzrr409yOnA0sL2qntS2rQQ+DKwFtgLHV9WPuopBd2dfviTo9oz/vcARM9o2AJuran9gc7suSRqjzgp/VX0W+OGM5mOBTe3yJuC4ro4vSZrduPv496uqbQDt675zbZhkfZItSbbcdNNNYwtQkpa7ib24W1Ubq2pdVa1btWrVUocjScvGuAv/jUlWA7Sv28d8fEnqvXEX/nOBk9rlk4Bzxnx8Seq9LodzngEcCuyT5HrgDcCpwJlJXgFcB7y4q+NLkrN8zq6zwl9VL5njrcO7OqYkabiJvbgrSeqGhV+SesZpmaeYUzBIWgjP+CWpZyz8ktQzdvVMAbt0JC0mz/glqWcs/JLUMxZ+SeoZ+/glTb1dvQ7W96kcPOOXpJ6x8EtSz1j4Jaln7ONfAn3vX5QWyntaFodn/JLUMxZ+SeoZu3o65J+lkiaRZ/yS1DMWfknqGQu/JPWMffwTxGsC0tLa1aHW0zo02zN+SeoZC78k9YyFX5J6xj5+SRpiuV1/84xfknrGwi9JPbPsu3rGMTxruf0ZKGn3LFbdmW+f3bEkZ/xJjkjyrSTXJNmwFDFIUl+NvfAnWQG8DXgecADwkiQHjDsOSeqrpTjjfxpwTVVdW1W3Ax8Cjl2COCSpl1JV4z1g8iLgiKr6t+36y4GnV9WrZmy3Hljfrj4O+NYiHH4f4OZF+JxJsFxyWS55gLlMouWSBywsl0dW1aqZjUtxcTeztN3j26eqNgIbF/XAyZaqWreYn7lUlksuyyUPMJdJtFzygMXNZSm6eq4HHjGw/nDghiWIQ5J6aSkK/yXA/kkeleQ+wInAuUsQhyT10ti7eqrqjiSvAj4FrABOr6qvj+nwi9p1tMSWSy7LJQ8wl0m0XPKARcxl7Bd3JUlLyykbJKlnLPyS1DNTX/iTbE3ytSSXJdnStq1MckGSq9vXvQe2f107VcS3kvzqQPtT28+5JslfJJlt2OlS5PLiJF9PcleSdTO2n8hc5sjjT5J8M8kVSc5O8nOTnsc8ubyxzeOyJJ9O8tBpzWXgvdckqST7DLRNZC5z/E5OSfL9tu2yJEdOeh5z5dK2/3Yb79eTnLbouVTVVP8AW4F9ZrSdBmxolzcAb2qXDwAuB+4LPAr4R2BF+97FwC/T3GfwSeB5E5LLE2huYLsQWDfQPrG5zJHHc4E92uU3Tfnv5EEDy/8ReMe05tK2P4JmsMV3d7w/ybnM8Ts5BXjNLNtObB7z5PIs4O+B+7br+y52LlN/xj+HY4FN7fIm4LiB9g9V1W1V9R3gGuBpSVbT/M/8xWr+Ff9mYJ8lVVVXVdVsdy1PVS5V9emquqNd/RLN/RswZXkAVNUtA6v3Z+cNiFOXS+vPgNdy9xsppzWXmaYxj38PnFpVtwFU1fa2fdFyWQ6Fv4BPJ7k0zTQPAPtV1TaA9nXftv1hwPcG9r2+bXtYuzyzfdxmy2Uuk5zLsDx+g+asBCY7D5gjlyT/Pcn3gJcCf9g2T10uSY4Bvl9Vl8/YdpJzmeu/r1e1XXCnZ2f37iTnAbPn8ljgmUm+nOSiJL/Uti9aLsthPv5DquqGJPsCFyT55jzbzjVdxEjTSIzBPXKpqs/Ose0k5zJnHklOBu4APtBuO8l5wBy5VNXJwMlJXge8CngDU5gLcDJNN9xMk5zLbHm8HXhjG8sbgbfQnGBMch4wey57AHsDzwB+CTgzyaNZxFym/oy/qm5oX7cDZ9PM/nlj++cP7euOP5Xmmi7ienZ2PQy2j9UcucxlYnOZK48kJwFHAy9t/ySFCc4DRvqdfBB4Ybs8bbn8K5q+4suTbG3j+kqShzDBucz2O6mqG6vqzqq6C3gXO39PE5sHzPnf1/XAx6pxMXAXzQRti5fLuC9mLOYPTf/qAweWvwAcAfwJd7+4e1q7/ETufnHkWnZeHLmE5ht2x8WRIychl4H3L+TuF3cnMpd5fidHAN8AVs3YfiLzGJLL/gPb/Dbw0WnNZcY2W9l5cXcic5nnd7J6YJvfoekLn9g8huTym8B/a9sfS9O9k8XMZWxJdvQP9+j2H+Jy4OvAyW37zwObgavb15UD+5xMczX8Wwxc+QbWAVe27/0l7V3NE5DL82m+0W8DbgQ+Ncm5zJPHNe1/wJe1P++Y5DyG5HJWG9cVwN8CD5vWXGZss5WBESaTmMs8v5P3AV9rfyfncvcvgonLY0gu9wHe38b2FeCwxc7FKRskqWemvo9fkrRrLPyS1DMWfknqGQu/JPWMhV+SesbCL0k9Y+HXstJOLfy+gfU9ktyU5LyOjndhO0XuMe36e5P83yQPHNjmrTOnPJ7lc96b5N/NaDsuySeS7NVO23v7fJ8hjcrCr+Xm/wBPSrJXu/4c4PsdH/OlVXXuwPo1NDMpkuReNNPsDovhDODEGW0nAmdU1a1VdRBLMKWAlicLv5ajTwJHtcsvoSmqACR5WpIvJPlq+/q4tv2JSS5uz6yvSLJ/kvsn+bsklye5MskJIx7/DGDHtocCn6eZmG5HDC8bONY7k6ygmX/98QNzTN0PeDbw8YX+I0hzsfBrOfoQcGKSPYGnAF8eeO+bwL+sqoNpplP+47b9N4G3tmfW62imyTgCuKGqDqyqJwHnj3j8q4FV7dTAL2njASDJE2i+FA5pj3UnzV8MdwIfA45vNz0G+ExV/XRXEpdGYeHXslNVVwBraYruJ2a8/WDgI0mupHkAyRPb9i8C/znJHwCPrKpbaeZ+eXaSNyV5ZlX9ZBfC+BhNV83TgX8YaD8ceCpwSZLL2vVHt+8NdvecyMBfKtJisvBruToXeDP3LJ5vpDmTfhLwa8CeAFX1QZqz7FuBTyU5rKq+TVOkvwb8jyR/yOg+1B7rgmqmCt4hwKaqOqj9eVxVndK+93lgdZIDgV/hnl9a0qJYDg9ikWZzOvCTqvpakkMH2h/Mzgut/2ZHY/ugi2ur6i/a5ae0D8X4YVW9P8k/DW4/TFVd1z505u9nvLUZOCfJn1XV9iQraabm/W5VVZIzaR4X+omq+uddSVgalWf8Wpaq6vqqeussb51Gc/b+eWDFQPsJwJVt98vjaZ5b+mTg4rbtZOCPdjGGd1bVP85o+wbweprH7V0BXACsHtjkDOBABq4LSIvNaZml3ZDkQuA1VbVlDMfaSvMwnpu7PpaWN8/4pd3zQ+C9O27g6sKOG7iAe9M8hk/aLZ7xS1LPeMYvST1j4ZeknrHwS1LPWPglqWf+H8uuvFhbx1IqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"test_data.csv\", names=[\"mass\"])\n",
    "\n",
    "num_bins = 50\n",
    "vals, bins, _ = plt.hist(df[\"mass\"], num_bins)\n",
    "bin_width = (bins[-1] - bins[0])/num_bins\n",
    "plt.xlabel(\"Mass [MeV]\")\n",
    "plt.ylabel(f\"Events / ({bin_width:.1f} MeV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to perform a fit to the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the results\n",
    "\n",
    "1. Plot the PDF with best-fit values overlaid on to a histogram of the data.\n",
    "  - _**Hint**: to achieve the same normalisation for the data and the PDF, you may choose to use `density=True` in the arguments to `pyplot.hist`_\n",
    "1. Plot a 'likelihood trace' (i.e. the 'history' of the likelihood at each iteration) for the following cases:\n",
    "  - Several choices of fixed step size for batch gradient descent\n",
    "  - The effect of different methods of gradient-descent (6 in total)\n",
    "    - batch\n",
    "    - mini-batch\n",
    "    - stochastic\n",
    "    - all of the above using the Nesterov technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
